Bootstrap: docker
From: nvidia/cuda:11.6.2-cudnn8-runtime-ubuntu20.04

%post
    # Set non-interactive mode
    export DEBIAN_FRONTEND=noninteractive
    
    # Update and install system dependencies
    apt-get update && apt-get install -y \
        python3.9 \
        python3.9-dev \
        python3-pip \
        wget \
        curl \
        git \
        vim \
        build-essential \
        libgl1-mesa-glx \
        libglib2.0-0 \
        libsm6 \
        libxext6 \
        libxrender-dev \
        libgomp1 \
        libglu1-mesa \
        && rm -rf /var/lib/apt/lists/*
    
    # Make python3.9 the default
    update-alternatives --install /usr/bin/python python /usr/bin/python3.9 1
    update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1
    
    # Upgrade pip
    python -m pip install --upgrade pip setuptools wheel
    
    # CRITICAL: Install NumPy 1.x FIRST before anything else
    pip install --no-cache-dir numpy==1.23.5
    
    # Verify NumPy installation
    python -c "import numpy; print(f'NumPy installed: {numpy.__version__}')"
    
    # Install PyTorch with CUDA 11.6 support
    pip install --no-cache-dir \
        torch==1.13.0+cu116 \
        torchvision==0.14.0+cu116 \
        -f https://download.pytorch.org/whl/torch_stable.html
    
    # Verify PyTorch installation
    python -c "
import torch
import numpy as np
print(f'PyTorch: {torch.__version__}')
print(f'NumPy: {np.__version__}')
print(f'CUDA: {torch.cuda.is_available()}')
"
    
    # Install PyTorch Geometric dependencies with specific versions
    pip install --no-cache-dir \
        torch-scatter==2.1.0 \
        torch-sparse==0.6.16 \
        torch-cluster==1.6.0 \
        torch-spline-conv==1.2.1 \
        -f https://data.pyg.org/whl/torch-1.13.0+cu116.html
    
    # Install PyTorch Geometric
    pip install --no-cache-dir torch-geometric==2.2.0
    
    # Install pytorch3d (for better loss functions)
    pip install --no-cache-dir \
        "git+https://github.com/facebookresearch/pytorch3d.git@v0.7.2"
    
    # Install other required packages
    pip install --no-cache-dir \
        scipy \
        scikit-learn \
        matplotlib \
        tqdm \
        tensorboard \
        Pillow \
        open3d==0.17.0 \
        einops \
        lmdb \
        pyyaml \
        wandb \
        h5py
    
    # Final verification
    python -c "
import numpy as np
import torch
import torch_geometric
from torch_geometric.nn import GCNConv

print('=== Package Versions ===')
print(f'NumPy: {np.__version__}')
print(f'PyTorch: {torch.__version__}')
print(f'PyTorch Geometric: {torch_geometric.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')

# Test mixed precision
from torch.cuda.amp import autocast, GradScaler
print('Mixed precision support: OK')

# Test DDP
import torch.distributed as dist
print('Distributed training support: OK')

assert np.__version__.startswith('1.'), f'NumPy version error: {np.__version__}'
print('âœ“ All imports successful!')
"
    
    # Create mount directories
    mkdir -p /workspace /data /checkpoints /visuals /experiments

%environment
    export PYTHONPATH=/workspace:$PYTHONPATH
    export CUDA_HOME=/usr/local/cuda
    export PATH=$CUDA_HOME/bin:$PATH
    export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
    export OMP_NUM_THREADS=4
    # Enable TF32 for A100
    export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
    # Better NCCL settings
    export NCCL_DEBUG=WARN
    export NCCL_SOCKET_IFNAME=^docker0,lo

%runscript
    echo "GCNN PCR Container - A100 Optimized"
    echo "===================================="
    python -c "
import torch, numpy, torch_geometric
print(f'NumPy: {numpy.__version__}')
print(f'PyTorch: {torch.__version__}')
print(f'PyTorch Geometric: {torch_geometric.__version__}')
print(f'CUDA: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'GPU Count: {torch.cuda.device_count()}')
"
    exec "$@"

%labels
    Author GCNNPCR Team
    Version 3.0
    Description A100-optimized build with mixed precision and distributed training support